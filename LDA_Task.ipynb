{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T21:27:18.577068Z","iopub.execute_input":"2022-04-10T21:27:18.577556Z","iopub.status.idle":"2022-04-10T21:27:18.593894Z","shell.execute_reply.started":"2022-04-10T21:27:18.577499Z","shell.execute_reply":"2022-04-10T21:27:18.593013Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing important modules","metadata":{}},{"cell_type":"code","source":"import nltk \nnltk.download('stopwords')\nimport re \nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\n#Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n#Spacy for lemmatization\nimport spacy \n\n#plotting tools \nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Enable logging for gensim \nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\" , category=DeprecationWarning)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:18.599514Z","iopub.execute_input":"2022-04-10T21:27:18.600201Z","iopub.status.idle":"2022-04-10T21:27:23.775833Z","shell.execute_reply.started":"2022-04-10T21:27:18.600163Z","shell.execute_reply":"2022-04-10T21:27:23.774862Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Prepare Stopwords**\nWe have already downloaded the stopwords. Let’s import them and make it available in stop_words","metadata":{}},{"cell_type":"code","source":"#NLTK stopwords\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:23.777207Z","iopub.execute_input":"2022-04-10T21:27:23.777456Z","iopub.status.idle":"2022-04-10T21:27:23.784274Z","shell.execute_reply.started":"2022-04-10T21:27:23.777408Z","shell.execute_reply":"2022-04-10T21:27:23.783453Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install openpyxl\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:23.786953Z","iopub.execute_input":"2022-04-10T21:27:23.788729Z","iopub.status.idle":"2022-04-10T21:27:32.420635Z","shell.execute_reply.started":"2022-04-10T21:27:23.788573Z","shell.execute_reply":"2022-04-10T21:27:32.419609Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#importing Dataset\ndf = pd.read_excel('../input/dataset/Pubmed5k.xlsx')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:32.422417Z","iopub.execute_input":"2022-04-10T21:27:32.422691Z","iopub.status.idle":"2022-04-10T21:27:33.371410Z","shell.execute_reply.started":"2022-04-10T21:27:32.422659Z","shell.execute_reply":"2022-04-10T21:27:33.370485Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(df.Title.unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:33.372870Z","iopub.execute_input":"2022-04-10T21:27:33.373078Z","iopub.status.idle":"2022-04-10T21:27:33.381385Z","shell.execute_reply.started":"2022-04-10T21:27:33.373053Z","shell.execute_reply":"2022-04-10T21:27:33.380461Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**cleaning the data**","metadata":{}},{"cell_type":"code","source":"#convert to list \ndata = df.Abstract.values.tolist()\n\n#removing e-mails if exists\ndata = [re.sub('\\S*@\\S*\\s' , '',sent) for sent in data]\n\n#remove newline chars\ndata = [re.sub('\\s+' , ' ' , sent) for sent in data]\n\n#Remove distracting single quotes \ndata = [re.sub(\"\\'\" , \"\" , sent) for sent in data]\n\npprint(data[:1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:33.382557Z","iopub.execute_input":"2022-04-10T21:27:33.382775Z","iopub.status.idle":"2022-04-10T21:27:34.561283Z","shell.execute_reply.started":"2022-04-10T21:27:33.382747Z","shell.execute_reply":"2022-04-10T21:27:34.560095Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize words and clean-up text\n**Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.**","metadata":{}},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:34.562592Z","iopub.execute_input":"2022-04-10T21:27:34.562894Z","iopub.status.idle":"2022-04-10T21:27:39.678449Z","shell.execute_reply.started":"2022-04-10T21:27:34.562864Z","shell.execute_reply":"2022-04-10T21:27:39.677566Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Creating Bigram and Trigram Models","metadata":{}},{"cell_type":"code","source":"#building the bigram and trigram models \nbigram = gensim.models.Phrases(data_words , min_count=5 , threshold=100)\ntrigram = gensim.models.Phrases(bigram[data_words] , threshold=100)\n\n#faster way to get sentence clubbed as trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n#see trigram example \nprint(trigram_mod[bigram_mod[data_words[0]]])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:39.679647Z","iopub.execute_input":"2022-04-10T21:27:39.679889Z","iopub.status.idle":"2022-04-10T21:27:50.243041Z","shell.execute_reply.started":"2022-04-10T21:27:39.679860Z","shell.execute_reply":"2022-04-10T21:27:50.242259Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Remove Stopwords, Make Bigrams and Lemmatize\nThe bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially.","metadata":{}},{"cell_type":"code","source":"#define functions for stopwords , bigrams , trigrams , lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts , allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out        ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:50.244526Z","iopub.execute_input":"2022-04-10T21:27:50.244870Z","iopub.status.idle":"2022-04-10T21:27:50.254366Z","shell.execute_reply.started":"2022-04-10T21:27:50.244826Z","shell.execute_reply":"2022-04-10T21:27:50.253327Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Let's call the functions in order to perform the operations**","metadata":{}},{"cell_type":"code","source":"#remove stopwords\ndata_words_nostops = remove_stopwords(data_words)\n\n#forming bi-grams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en_core_web_sm' , disable=['parser', 'ner'])\n\n#do lemmatizayion keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:27:50.255804Z","iopub.execute_input":"2022-04-10T21:27:50.256422Z","iopub.status.idle":"2022-04-10T21:29:07.008124Z","shell.execute_reply.started":"2022-04-10T21:27:50.256389Z","shell.execute_reply":"2022-04-10T21:29:07.007114Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Create the Dictionary and Corpus needed for Topic Modeling","metadata":{}},{"cell_type":"code","source":"#create Dictionary \nid2word = corpora.Dictionary(data_lemmatized)\n\n#ceate corpus\ntexts = data_lemmatized\n\n#term document frequency \ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:29:07.010060Z","iopub.execute_input":"2022-04-10T21:29:07.010379Z","iopub.status.idle":"2022-04-10T21:29:08.465724Z","shell.execute_reply.started":"2022-04-10T21:29:07.010335Z","shell.execute_reply":"2022-04-10T21:29:08.464688Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"> **Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).**\n\n> **For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.**","metadata":{}},{"cell_type":"code","source":"# readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:29:08.486989Z","iopub.execute_input":"2022-04-10T21:29:08.487311Z","iopub.status.idle":"2022-04-10T21:29:08.502388Z","shell.execute_reply.started":"2022-04-10T21:29:08.487268Z","shell.execute_reply":"2022-04-10T21:29:08.501463Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **Building The Topic Model**\n***Let's try number of topics to be 10 then consider the optimal number of topics***","metadata":{}},{"cell_type":"code","source":"#Build LDA Model \nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10,\n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:29:08.504495Z","iopub.execute_input":"2022-04-10T21:29:08.504802Z","iopub.status.idle":"2022-04-10T21:29:51.564603Z","shell.execute_reply.started":"2022-04-10T21:29:08.504760Z","shell.execute_reply":"2022-04-10T21:29:51.563662Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **View The Topics in LDA Model**\n**the above model is built with 10 different topics where each topic is consists of variety of keywords each of these keyword has its certain weight that contributes to the topic**\n\n***let's see in the next cell the importance score of the keyword for each topic***","metadata":{}},{"cell_type":"code","source":"#print the keyword in the 10 topics \npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:29:51.565799Z","iopub.execute_input":"2022-04-10T21:29:51.566042Z","iopub.status.idle":"2022-04-10T21:29:51.587253Z","shell.execute_reply.started":"2022-04-10T21:29:51.566013Z","shell.execute_reply":"2022-04-10T21:29:51.586218Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **Compute Model Perplexity and Coherence Score**\n\n***Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.***","metadata":{}},{"cell_type":"code","source":"#Compute Perplexity\n#a measure of how good the model is , more lower more better \nprint('\\nPerplexity: ' , lda_model.log_perplexity(corpus))\n\n#Compute Coherence Score \ncoherence_model_lda = CoherenceModel(model=lda_model , texts=data_lemmatized , dictionary=id2word , coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ' , coherence_lda)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:29:51.589179Z","iopub.execute_input":"2022-04-10T21:29:51.589395Z","iopub.status.idle":"2022-04-10T21:30:01.518312Z","shell.execute_reply.started":"2022-04-10T21:29:51.589369Z","shell.execute_reply":"2022-04-10T21:30:01.517247Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Let's Now Visualize The Topics-Keywords**\n\n***Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks.***","metadata":{}},{"cell_type":"code","source":"    \n\n        #Visualize The Topics \npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model , corpus , id2word)\nvis","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:30:01.520281Z","iopub.execute_input":"2022-04-10T21:30:01.520782Z","iopub.status.idle":"2022-04-10T21:30:06.586298Z","shell.execute_reply.started":"2022-04-10T21:30:01.520731Z","shell.execute_reply":"2022-04-10T21:30:06.585371Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"So how to infer pyLDAvis’s output?\n\nEach bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n\nA good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n\nA model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n\nAlright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\nWe have successfully built a good looking topic model.\n\nGiven our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n\nUpnext, we will improve upon this model by using Mallet’s version of LDA algorithm and then we will focus on how to arrive at the optimal number of topics given any large corpus of text","metadata":{}},{"cell_type":"markdown","source":"# **How to find the optimal number of topics for LDA?**\n\n***My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.***\n","metadata":{}},{"cell_type":"code","source":"def compute_coherence_values(dictionary , corpus , texts , limit , start = 2 , step = 1):\n    \"\"\"\n    compute coherence score for various number of topics \n    \n    parameters :\n    ------------\n    - dictionnary : Gensim Dictionary \n    - corpus : gensim corpus \n    - texts : list of input texts \n    - limit : maximum number of topics\n    \n    returns:\n    --------\n    - model list :List of LDA topic Models \n    - coherence_values : coherence values corresponding to the lda model with respective number of topics \n    \"\"\"\n    x = 1\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start , limit , step):\n        model =  gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=x,\n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model , texts=texts , dictionary=dictionary , coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n        x = x+1\n        \n    return model_list , coherence_values\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:30:06.588204Z","iopub.execute_input":"2022-04-10T21:30:06.589071Z","iopub.status.idle":"2022-04-10T21:30:06.601118Z","shell.execute_reply.started":"2022-04-10T21:30:06.589013Z","shell.execute_reply":"2022-04-10T21:30:06.599991Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Can take a long time to run.\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:30:06.602929Z","iopub.execute_input":"2022-04-10T21:30:06.603785Z","iopub.status.idle":"2022-04-10T21:44:58.475483Z","shell.execute_reply.started":"2022-04-10T21:30:06.603732Z","shell.execute_reply":"2022-04-10T21:44:58.474394Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#show graph\nx = range(2 , 20 , 1)\nplt.plot(x , coherence_values)\nplt.xlabel(\"Num of topics\")\nplt.ylabel(\"Coherence Score\")\nplt.legend((\"coherence_values\") , loc='best')\nplt.title(\"Chossing the optimal model with coherence score\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:44:58.478597Z","iopub.execute_input":"2022-04-10T21:44:58.478994Z","iopub.status.idle":"2022-04-10T21:44:58.725635Z","shell.execute_reply.started":"2022-04-10T21:44:58.478944Z","shell.execute_reply":"2022-04-10T21:44:58.724668Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:44:58.727127Z","iopub.execute_input":"2022-04-10T21:44:58.727455Z","iopub.status.idle":"2022-04-10T21:44:58.744111Z","shell.execute_reply.started":"2022-04-10T21:44:58.727395Z","shell.execute_reply":"2022-04-10T21:44:58.743165Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# **As required**\n**i'll continue using top 3 topics**","metadata":{}},{"cell_type":"code","source":"optimal_model = model_list[2]\nmodel_topics = optimal_model.show_topics(formatted=False)\npprint(optimal_model.print_topics(num_words=10))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:44:58.745402Z","iopub.execute_input":"2022-04-10T21:44:58.745741Z","iopub.status.idle":"2022-04-10T21:44:58.758976Z","shell.execute_reply.started":"2022-04-10T21:44:58.745703Z","shell.execute_reply":"2022-04-10T21:44:58.757862Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Let's buld the model with the required number of topics\n**and visulaize it**","metadata":{}},{"cell_type":"code","source":"#Build LDA Model \nlda_model_opt = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=3,\n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:44:58.760234Z","iopub.execute_input":"2022-04-10T21:44:58.760490Z","iopub.status.idle":"2022-04-10T21:45:38.811324Z","shell.execute_reply.started":"2022-04-10T21:44:58.760461Z","shell.execute_reply":"2022-04-10T21:45:38.810338Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"pprint(lda_model_opt.print_topics())\ndoc_lda = lda_model_opt[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:45:38.812823Z","iopub.execute_input":"2022-04-10T21:45:38.814118Z","iopub.status.idle":"2022-04-10T21:45:38.825273Z","shell.execute_reply.started":"2022-04-10T21:45:38.814074Z","shell.execute_reply":"2022-04-10T21:45:38.824191Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# **Let's again Compute Model Perplexity and Coherence Score**\n***Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.***","metadata":{}},{"cell_type":"code","source":"#Compute Perplexity\n#a measure of how good the model is , more lower more better \nprint('\\nPerplexity: ' , lda_model_opt.log_perplexity(corpus))\n\n#Compute Coherence Score \ncoherence_model_lda_opt = CoherenceModel(model=lda_model_opt , texts=data_lemmatized , dictionary=id2word , coherence='c_v')\ncoherence_lda_opt = coherence_model_lda_opt.get_coherence()\nprint('\\nCoherence Score: ' , coherence_lda_opt)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:45:38.827049Z","iopub.execute_input":"2022-04-10T21:45:38.827649Z","iopub.status.idle":"2022-04-10T21:45:45.829660Z","shell.execute_reply.started":"2022-04-10T21:45:38.827600Z","shell.execute_reply":"2022-04-10T21:45:45.828545Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# **Let's Now Visualize The Topics-Keywords**\n\n***Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks.***","metadata":{}},{"cell_type":"code","source":"#Visualize The Topics \npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model_opt , corpus , id2word)\nvis","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:45:45.831906Z","iopub.execute_input":"2022-04-10T21:45:45.832275Z","iopub.status.idle":"2022-04-10T21:45:50.806612Z","shell.execute_reply.started":"2022-04-10T21:45:45.832228Z","shell.execute_reply":"2022-04-10T21:45:50.805675Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Let's Find out the dominant topic in each sentence\n**One of the practical application of topic modeling is to determine what topic a given document is about.**\n\n**To find that, we find the topic number that has the highest percentage contribution in that document.**","metadata":{}},{"cell_type":"code","source":"def format_topics_sentences(ldamodel = lda_model_opt , corpus = corpus, texts=data):\n    #Init output\n    sent_topics_df = pd.DataFrame()\n    \n\n    #Get The Main Topic in Each Document\n    for i , row in enumerate(ldamodel[corpus]):\n        row2 = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n        #Get The Dominant Topic , Perc% Contibution and keywords for each document \n        for j , (topic_num , prop_topic) in enumerate(row2):\n            if j == 0: # Dominant Topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word , prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num) , round(prop_topic,4) , topic_keywords]) , ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n    \n    #Add original text in the end of output \n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df , contents] , axis=1)\n    return (sent_topics_df)\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_opt , corpus=corpus , texts=data)\n\n#Formatting \ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T21:45:50.808746Z","iopub.execute_input":"2022-04-10T21:45:50.809122Z","iopub.status.idle":"2022-04-10T21:46:16.969038Z","shell.execute_reply.started":"2022-04-10T21:45:50.809088Z","shell.execute_reply":"2022-04-10T21:46:16.968098Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**\n\n> **First things first i want to thank everyone that give me the honor to proceed to this step**\n\n> **Second I really hope this simple NoteBook Meet your Expectations**\n\n> **And I really happy to say i'm really honord by this chance , whatever the result you see i accept it happily**\n\n# **About the next steps to improve The Model**\n> **We Could Expand The Number Of Topics To get More Better Coherence Score**\n\n> **Expand The DataSet And Get More Organised Documents For Better Results**\n\n> ***Finally I want To Say SomeThing .. Maybe This Work Isn't Ideal For Your Needs , But to Be Honest This is The First Time For Me To work With This Algorithm(LDA) , Shame On Me Ofcourse , But I really Tried My Best To Learn With The Fly To Get The Best Results as much as i can , I really happy to deal with new algorithm and of course i'll add this valuable Task To my Portfolio***","metadata":{}}]}